{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test optimization of minimization of $\\log^2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(2, name='x', dtype=tf.float32)\n",
    "log_x = tf.log(x)\n",
    "log_x_squared = tf.square(log_x)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(log_x_squared)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "def optimize():\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        print(\"starting at\", \"x:\", session.run(x), \"log(x)^2:\", session.run(log_x_squared))\n",
    "        for step in range(10):  \n",
    "            session.run(train)\n",
    "            print(\"step\", step, \"x:\", session.run(x), \"log(x)^2:\", session.run(log_x_squared))\n",
    "\n",
    "optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test placeholders usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(1., name='x')\n",
    "a = tf.placeholder(dtype=float, shape=(), name='a')\n",
    "\n",
    "def cost():\n",
    "    return tf.add(tf.square(x),a)\n",
    "\n",
    "#train = tf.train.AdamOptimizer(0.5).minimize(cost())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "def optimize():\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        print(session.run(cost(), feed_dict={x: 5.0, a :51.0}))\n",
    "\n",
    "\n",
    "optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Erdos-Renyi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import with_statement\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import networkqit as nq\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from networkqit import graph_laplacian as GL\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "########## INITIALIZATION PHASE ##########\n",
    "N = 100\n",
    "pstar = 0.2\n",
    "beta = 0.001\n",
    "shape=[N, N]\n",
    "G = nx.erdos_renyi_graph(N,pstar)\n",
    "A = nx.to_numpy_array(G)\n",
    "print('Empirical density:', nx.density(G))\n",
    "L=nq.graph_laplacian(A)\n",
    "rho = nq.compute_vonneuman_density(L=L,beta=beta)\n",
    "lo = np.linalg.eigvalsh(L)\n",
    "\n",
    "########## TENSORFLOW PHASE ##########\n",
    "with tf.device('/device:CPU:0'):\n",
    "    # Define the tensorflow optimization variable p\n",
    "    p = tf.Variable(float(np.random.random()), name='p')\n",
    "    \n",
    "    # Convert stuff to tensorflow to create nodes in the computational graph\n",
    "    beta=tf.convert_to_tensor(beta,name='beta',dtype=tf.float32)\n",
    "    rho = tf.convert_to_tensor(rho,name='rho',dtype=tf.float32)\n",
    "    Lobs = tf.convert_to_tensor(L,name='Lobs',dtype=tf.float32)\n",
    "    lo = tf.convert_to_tensor(lo,dtype=tf.float32)\n",
    "    \n",
    "    # Create the sampling matrix\n",
    "    rij = tf.random_uniform(shape,minval=0.0,maxval=1.0)\n",
    "    rijsym = (tf.transpose(rij) + rij) / 2.0\n",
    "    pij = tf.multiply(rijsym,tf.ones([N,N])-tf.eye(N))\n",
    "    \n",
    "    # Sample from the probabilities\n",
    "    Amodel = 1/(1 + tf.exp(40*pij-40*p)) # this is like setting Amodel = pij > p as the sigmoid is similar \n",
    "    Amodel = tf.multiply(Amodel,tf.constant([1.0]) - tf.eye(shape[0])) # set the diagonal to zero\n",
    "    model_density = tf.reduce_sum(Amodel) / (N * (N - 1))\n",
    "    \n",
    "    Lmodel = tf.diag(tf.reduce_sum(Amodel,axis=0)) - Amodel\n",
    "    Em = tf.reduce_sum(tf.multiply(Lmodel,rho))\n",
    "    # Observation energy\n",
    "    Eo = tf.reduce_sum(tf.multiply(Lobs,rho))\n",
    "    # Model laplacian eigenvalues\n",
    "    lm = tf.self_adjoint_eig(Lmodel)[0]\n",
    "    # Observation laplacian eigenvalues\n",
    "    #lo = tf.self_adjoint_eig(Lobs)[0]\n",
    "    # Model free energy\n",
    "    Fm = - tf.reduce_logsumexp(-beta*lm) / beta\n",
    "    # Observation free energy\n",
    "    Fo = - tf.reduce_logsumexp(-beta*lo) / beta\n",
    "    # Model loglikelihood\n",
    "    loglike = beta*(-Fm + Em)\n",
    "    # Observation entropy\n",
    "    entropy = beta*(-Fo + Eo)\n",
    "    # Model relative entropy (using abs for precision purposes)\n",
    "    rel_entropy = tf.abs(loglike - entropy)\n",
    "    grad = tf.gradients(rel_entropy,p)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    # Define the function to start optimization\n",
    "    train = optimizer.minimize(rel_entropy)\n",
    "    # Initialize the global variables\n",
    "    #print(tf.trainable_variables())\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "        session.run(init)\n",
    "        all_loss,all_theta = [],[]\n",
    "        epochs = 10\n",
    "        print('Initial p:', session.run(p))\n",
    "        for step in range(epochs):\n",
    "            session.run(train)\n",
    "            all_theta.append(session.run(model_density))\n",
    "            all_loss.append(session.run(rel_entropy))\n",
    "            g = session.run(grad)\n",
    "            print('\\r beta:',session.run(beta), 'step', step, 'p:',all_theta[-1],  'density:',session.run(model_density), 'loss:', all_loss[-1],'grad:',g, end='')\n",
    "\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(session.run(1-Amodel),cmap='binary')\n",
    "        plt.grid(False)\n",
    "        fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "        ax[0].plot(all_loss)\n",
    "        ax[1].plot(all_theta)\n",
    "        ax[1].plot([pstar]*len(all_theta),'r-')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the configuration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import with_statement\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "tf_logdir = '/home/carlo/workspace/networkqit/tensorboard/' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import networkqit as nq\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with g.as_default(), tf.device('/device:CPU:0'):\n",
    "    # Define the observed matrix (using the karate club)\n",
    "    N = 50\n",
    "    beta = 1*1E-2\n",
    "    #G = nx.erdos_renyi_graph(N,pstar)\n",
    "    #G = nx.karate_club_graph()\n",
    "    #G = nx.planted_partition_graph(4,10,0.8,0.2)\n",
    "    G = nx.barabasi_albert_graph(100,15)\n",
    "    print(nx.density(G))\n",
    "    avgnn = np.array(list(nx.neighbor_degree.average_neighbor_degree(G)))\n",
    "    N = len(G.nodes())\n",
    "    A = nx.to_numpy_array(G)\n",
    "    shape=[N, N]\n",
    "    m = A.sum()\n",
    "    L=np.diag(A.sum(axis=0))-A\n",
    "    rho = nq.compute_vonneuman_density(L=L,beta=beta)\n",
    "    \n",
    "    # Convert stuff to tensorflow to create nodes in the computational graph\n",
    "    beta=tf.convert_to_tensor(beta,name='beta',dtype=tf.float64)\n",
    "    rho = tf.convert_to_tensor(rho,name='rho',dtype=tf.float64)\n",
    "    Lobs = tf.convert_to_tensor(L,name='Lobs',dtype=tf.float64)\n",
    "    \n",
    "    # Define the tensorflow optimization variable x\n",
    "    xi = tf.Variable(tf.random_uniform(shape=[N,],dtype=tf.float64), name='xi',dtype=tf.float64)\n",
    "    # sampling phase, create the sampled matrix Amodel\n",
    "    rij = tf.random_uniform(shape,minval=0.0, maxval=1.0,dtype=tf.float64)\n",
    "    rij = (tf.transpose(rij)+rij) / 2.0\n",
    "    rij = tf.multiply(rij,tf.constant([1.0],dtype=tf.float64)-tf.eye(N,dtype=tf.float64))\n",
    "    \n",
    "    xij = tf.einsum('i,j->ij', xi, xi) # outer product\n",
    "    pij = xij / (1.0 + xij)\n",
    "    Amodel = 1.0 / (1.0 + tf.exp(50*rij-50*pij)) # this is like setting < to 0 and > to 1\n",
    "    Amodel = tf.multiply(Amodel,tf.constant([1.0],dtype=tf.float64)-tf.eye(shape[0],dtype=tf.float64)) # set diagonal to zero\n",
    "    # Compute the model density\n",
    "    model_density = tf.reduce_sum(Amodel)/(N * (N-1))\n",
    "    #Amodel = bernoulli_adj(p,[N,N])\n",
    "    #Amodel = tf.distributions.Bernoulli(probs=tf.constant(p,shape=[N,N]))\n",
    "    \n",
    "    Lmodel = tf.diag(tf.reduce_sum(Amodel,axis=0)) - Amodel\n",
    "    deltaL = m - tf.reduce_sum(Amodel)\n",
    "    # Model energy\n",
    "\n",
    "    Em = tf.reduce_sum(tf.multiply(Lmodel,rho,name='Lmrho'),name='TrLmrho')\n",
    "    # Observation energy\n",
    "    Eo = tf.reduce_sum(tf.multiply(Lobs,rho,name='Lobsrho'),name='TrL_orho')\n",
    "    # Model laplacian eigenvalues\n",
    "    lm = tf.linalg.eigvalsh(Lmodel,name='lambda_model')\n",
    "    # Observation laplacian eigenvalues\n",
    "    lo = tf.linalg.eigvalsh(Lobs,name='lambda_obs')\n",
    "    # Model free energy\n",
    "    Fm = -tf.reduce_logsumexp(-beta*lm,name='Fm') / beta\n",
    "    # Observation free energy\n",
    "    Fo = -tf.reduce_logsumexp(-beta*lo,name='Fo') / beta\n",
    "\n",
    "    loglike = beta*(-Fm + Em)\n",
    "    entropy = beta*(-Fo + Eo)\n",
    "    rel_entropy = tf.abs(loglike - entropy)\n",
    "    grad = tf.gradients(rel_entropy,xi)\n",
    "\n",
    "    # Define the optimizer\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(rel_entropy)\n",
    "    # Initialize the global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar('loss_logger', rel_entropy)\n",
    "    tf.summary.scalar('deltam_logger', deltaL)\n",
    "    tf.summary.histogram('xi_logger',xi)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # Run the computational graph\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "        # op to write logs to Tensorboard\n",
    "        writer = tf.summary.FileWriter(tf_logdir, session.graph)\n",
    "        session.run(init)\n",
    "        all_loss,alldeltaL = [], []\n",
    "        epochs = 2000\n",
    "        ##### Start the training\n",
    "        for step in range(epochs):\n",
    "            ########## Tensorflow logging ##########\n",
    "            session.run(train)\n",
    "            summary = session.run(merged_summary_op)\n",
    "            #print(summary,step)\n",
    "            writer.add_summary(summary, step)\n",
    "            \n",
    "            ####### Matplotlib logging #######\n",
    "            all_loss.append(session.run(rel_entropy))\n",
    "            alldeltaL.append(session.run(deltaL))\n",
    "            # Write logs at every iteration\n",
    "            #summary_writer.add_summary(summary, step )\n",
    "            writer.flush()\n",
    "            print('\\r beta:',session.run(beta), 'step', step,  'density:',session.run(model_density), 'deltam:', session.run(deltaL) ,'loss:', all_loss[-1],end='')\n",
    "        writer.close()\n",
    "        fig,ax = plt.subplots(nrows=2,ncols=3,figsize=(20,9))\n",
    "        ax[0,0].plot(all_loss)\n",
    "        ax[0,0].set_title('Relative entropy')\n",
    "        ax[0,0].set_xlabel('iteration')\n",
    "        ax[0,1].plot(A.sum(axis=0),'r-')\n",
    "        ax[0,1].set_title('Degree')\n",
    "        ax[0,1].set_xlabel('node')\n",
    "        ax[0,1].set_ylabel('degree')\n",
    "        ax[0,1].plot(session.run(Amodel).sum(axis=0),'b.')\n",
    "        ax[0,2].plot(alldeltaL)\n",
    "        ax[0,2].set_title('$\\Delta m$')\n",
    "        \n",
    "        ax[1,0].plot(range(0,N),avgnn,'r-')\n",
    "        ax[1,0].plot(range(0,N),np.array(list(nx.neighbor_degree.average_neighbor_degree(nx.from_numpy_array(session.run(Amodel))))),'b.')\n",
    "        ax[1,0].set_title('Average neighbor degree')\n",
    "        ax[1,0].set_xlabel('node')\n",
    "        ax[1,0].set_ylabel('avg.neigh.deg')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the configuration model using placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import with_statement\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "tf_logdir = '/home/carlo/workspace/networkqit/tensorboard/' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import networkqit as nq\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "### Tensorflow stuff\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with tf.device('/device:CPU:0'):\n",
    "    \n",
    "    N = 50\n",
    "    beta = 0.2 * 1E-3\n",
    "    G = nx.barabasi_albert_graph(N,5)\n",
    "    A = nx.to_numpy_array(G)\n",
    "    shape=[N, N]\n",
    "    Lobs = nq.graph_laplacian(A)\n",
    "        \n",
    "    xi =  tf.Variable(tf.random_uniform(shape=[N,]), name='xi')\n",
    "    # sampling phase, create the random probabilities of sampling R\n",
    "    rij = tf.random_uniform(shape,minval=0.0, maxval=1.0)\n",
    "    rij = (tf.transpose(rij) + rij) / 2.0\n",
    "    rij = tf.multiply(rij,tf.constant([1.0])-tf.eye(shape[0]))\n",
    "\n",
    "    xij = tf.einsum('i,j->ij', xi, xi) # outer product\n",
    "    pij = xij / (1.0 + xij)\n",
    "    # Create the model adjacency matrix by sampling from pij with probabilities rij\n",
    "    Amodel = 1.0 / (1.0 + tf.exp(50*rij-50*pij)) # this is like setting < to 0 and > to 1\n",
    "    # Set the diagonal to zero\n",
    "    Amodel = tf.multiply(Amodel,tf.constant([1.0])-tf.eye(shape[0])) # set diagonal to zero\n",
    "    # Compute the model Laplacian\n",
    "    tf_Lmodel = tf.diag(tf.reduce_sum(Amodel,axis=0)) - Amodel\n",
    "    \n",
    "    \n",
    "    # Model energy\n",
    "    Em = tf.reduce_sum(tf.multiply(tf_Lmodel,tf_rho,name='Lmrho'), name='TrLmrho')\n",
    "    # Observation energy\n",
    "    Eo = tf.reduce_sum(tf.multiply(tf_Lobs,tf_rho,name='Lobsrho'), name='TrL_orho')\n",
    "    # Model laplacian eigenvalues\n",
    "    lm = tf.linalg.eigvalsh(tf_Lmodel,name='lambda_model')\n",
    "    # Observation laplacian eigenvalues\n",
    "    lo = tf.linalg.eigvalsh(tf_Lobs,name='lambda_obs')\n",
    "    # Model free energy\n",
    "    Fm = - tf.reduce_logsumexp(-tf_beta*lm,name='Fm') / tf_beta\n",
    "    # Observation free energy\n",
    "    Fo = - tf.reduce_logsumexp(-tf_beta*lo,name='Fo') / tf_beta\n",
    "    # Loglikelihood and relative entropy to train\n",
    "    loglike = beta*(-Fm + Em)\n",
    "    entropy = beta*(-Fo + Eo)\n",
    "    rel_entropy = tf.abs(loglike - entropy)\n",
    "        \n",
    "    # Compute the model density\n",
    "    #model_density = tf.trace(Lmodel)/(N * (N-1))\n",
    "    #deltaL = np.trace(Lobs) - tf.reduce_sum(Amodel)\n",
    "\n",
    "    grad = tf.gradients(rel_entropy,xi)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    # Create a summary to monitor cost tensor\n",
    "    #tf.summary.scalar('loss_logger', rel_entropy)\n",
    "    #tf.summary.scalar('deltam_logger', deltaL)\n",
    "    #tf.summary.histogram('xi_logger',xi)\n",
    "        \n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss())\n",
    "        \n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "        # Operations to write logs to Tensorboard\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        #writer = tf.summary.FileWriter(tf_logdir, session.graph)\n",
    "        for beta in np.logspace(-2,2,10):\n",
    "            # Initialize the global variables\n",
    "            for epoch in range(0,100):\n",
    "                session.run(train,feed_dict={'beta': beta, 'rho': nq.compute_vonneuman_density(L=Lobs,beta=beta), 'Lobs': Lobs })\n",
    "                print(session.run(xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the configuration model with decreasing $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import with_statement\n",
    "\n",
    "tf_logdir = '/tmp/'\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.linalg import expm\n",
    "\n",
    "def graph_laplacian(A):\n",
    "    return np.diag(A.sum(axis=0))-A\n",
    "\n",
    "def vonneumann_density(L,beta):\n",
    "    rho = expm(-L*beta)\n",
    "    return rho/np.trace(rho)\n",
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with g.as_default(), tf.device('/device:CPU:0'):\n",
    "    # Define the observed matrix (using the karate club)\n",
    "    N = 50\n",
    "    beta = 1*1E-2\n",
    "    #G = nx.erdos_renyi_graph(N,pstar)\n",
    "    #G = nx.karate_club_graph()\n",
    "    #G = nx.planted_partition_graph(4,10,0.8,0.2)\n",
    "    G = nx.barabasi_albert_graph(100,15)\n",
    "    avgnn = np.array(list(nx.neighbor_degree.average_neighbor_degree(G)))\n",
    "    N = len(G.nodes())\n",
    "    A = nx.to_numpy_array(G)\n",
    "    shape=[N, N]\n",
    "    m = A.sum()\n",
    "    L=np.diag(A.sum(axis=0))-A\n",
    "    rho = vonneumann_density(L=L,beta=beta)\n",
    "    \n",
    "    # Convert stuff to tensorflow to create nodes in the computational graph\n",
    "    beta=tf.convert_to_tensor(beta,name='beta',dtype=tf.float64)\n",
    "    rho = tf.convert_to_tensor(rho,name='rho',dtype=tf.float64)\n",
    "    Lobs = tf.convert_to_tensor(L,name='Lobs',dtype=tf.float64)\n",
    "    \n",
    "    # Define the tensorflow optimization variable x\n",
    "    xi = tf.Variable(tf.random_uniform(shape=[N,],dtype=tf.float64), name='xi',dtype=tf.float64)\n",
    "    # sampling phase, create the sampled matrix Amodel\n",
    "    rij = tf.random_uniform(shape,minval=0.0, maxval=1.0,dtype=tf.float64)\n",
    "    rij = (tf.transpose(rij)+rij) / 2.0\n",
    "    rij = tf.multiply(rij,tf.constant([1.0],dtype=tf.float64)-tf.eye(N,dtype=tf.float64))\n",
    "    \n",
    "    xij = tf.einsum('i,j->ij', xi, xi) # outer product\n",
    "    pij = xij / (1.0 + xij)\n",
    "    Amodel = 1.0 / (1.0 + tf.exp(50*rij-50*pij)) # this is like setting < to 0 and > to 1\n",
    "    Amodel = tf.multiply(Amodel,tf.constant([1.0],dtype=tf.float64)-tf.eye(shape[0],dtype=tf.float64)) # set diagonal to zero\n",
    "    # Compute the model density\n",
    "    model_density = tf.reduce_sum(Amodel)/(N * (N-1))\n",
    "    # Compute the model Laplacian    \n",
    "    Lmodel = tf.diag(tf.reduce_sum(Amodel,axis=0)) - Amodel\n",
    "    # Monitor the difference in number of links between model and data\n",
    "    deltaL = m - tf.reduce_sum(Amodel)\n",
    "    \n",
    "    # Model energy\n",
    "    Em = tf.reduce_sum(tf.multiply(Lmodel,rho,name='Lmrho'),name='TrLmrho')\n",
    "    # Observation energy\n",
    "    Eo = tf.reduce_sum(tf.multiply(Lobs,rho,name='Lobsrho'),name='TrL_orho')\n",
    "    # Model laplacian eigenvalues\n",
    "    lm = tf.linalg.eigvalsh(Lmodel,name='lambda_model')\n",
    "    # Observation laplacian eigenvalues\n",
    "    lo = tf.linalg.eigvalsh(Lobs,name='lambda_obs')\n",
    "    # Model free energy\n",
    "    Fm = -tf.reduce_logsumexp(-beta*lm,name='Fm') / beta\n",
    "    # Observation free energy\n",
    "    Fo = -tf.reduce_logsumexp(-beta*lo,name='Fo') / beta\n",
    "\n",
    "    loglike = beta*(-Fm + Em)\n",
    "    entropy = beta*(-Fo + Eo)\n",
    "    rel_entropy = tf.abs(loglike - entropy)\n",
    "    grad = tf.gradients(rel_entropy,xi)\n",
    "\n",
    "    # Define the optimizer\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(rel_entropy)\n",
    "    # Initialize the global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Run the computational graph\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "        # op to write logs to Tensorboard\n",
    "        writer = tf.summary.FileWriter(tf_logdir, session.graph)\n",
    "        session.run(init)\n",
    "        all_loss,alldeltaL = [], []\n",
    "        for beta in np.logspace(2,-2,10):\n",
    "            epochs = 200\n",
    "            ##### Start the training\n",
    "            for step in range(epochs):\n",
    "                ########## Tensorflow logging ##########\n",
    "                session.run(train)\n",
    "\n",
    "                ####### Matplotlib logging #######\n",
    "                all_loss.append(session.run(rel_entropy))\n",
    "                alldeltaL.append(session.run(deltaL))\n",
    "                # Write logs at every iteration\n",
    "                print('\\r beta:',session.run(beta), 'step', step,  'density:',session.run(model_density), 'deltam:', session.run(deltaL) ,'loss:', all_loss[-1],end='')\n",
    "\n",
    "        fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(20,9))\n",
    "        ax[0].plot(all_loss)\n",
    "        ax[0].set_title('Relative entropy')\n",
    "        ax[0].set_xlabel('iteration')\n",
    "        ax[1].plot(A.sum(axis=0),'r-')\n",
    "        ax[1].set_title('Degree')\n",
    "        ax[1].set_xlabel('node')\n",
    "        ax[1].set_ylabel('degree')\n",
    "        ax[1].plot(session.run(Amodel).sum(axis=0),'b.')\n",
    "        ax[2].plot(alldeltaL)\n",
    "        ax[2].set_title('$\\Delta m$')\n",
    "        \n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import with_statement\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.linalg import expm\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.Variable(2.0,dtype=tf.float64)\n",
    "# Initialize the global variables\n",
    "init = tf.global_variables_initializer()\n",
    "        \n",
    "def log_x(beta):\n",
    "    return (x-beta)**2\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for npbeta in np.logspace(2,-2,5):\n",
    "        beta=tf.convert_to_tensor(npbeta,name='beta',dtype=tf.float64)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(log_x(beta))\n",
    "        print(sess.run(optimizer))\n",
    "        #print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.Variable(2.0,dtype=tf.float64)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    def f(a):\n",
    "        return (x-a)**2\n",
    "    for aval in range(0,4):\n",
    "        \n",
    "        a =  tf.convert_to_tensor(aval,name='a',dtype=tf.float64)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(f(beta))\n",
    "        \n",
    "        for epochs in range(0,10):\n",
    "            sess.run(optimizer)\n",
    "        print(sess.run(x))\n",
    "        # How to initialize x for the next value of a using the current x?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import with_statement\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import networkqit as nq\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from networkqit import graph_laplacian as GL\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Imperative part\n",
    "G = nx.barabasi_albert_graph(200,5)\n",
    "N = len(G.nodes())\n",
    "shape=[N, N]\n",
    "A = nx.to_numpy_array(G)\n",
    "m = A.sum()\n",
    "L= np.diag(A.sum(axis=0)) - A\n",
    "print(nx.density(G))\n",
    "# Deferred part\n",
    "with tf.device('/device:CPU:0'):\n",
    "    Lobs = tf.placeholder(dtype=tf.float64)\n",
    "    beta = tf.placeholder(dtype=tf.float64)\n",
    "    rho = tf.linalg.expm(-beta*Lobs)/tf.trace(tf.linalg.expm(-beta*Lobs))\n",
    "\n",
    "    # Define the tensorflow optimization variable x\n",
    "    xi = tf.Variable(tf.random_uniform(shape=[N,],dtype=tf.float64), name='xi',dtype=tf.float64)\n",
    "    # sampling phase, create the sampled matrix Amodel\n",
    "    rij = tf.random_uniform(shape,minval=0.0, maxval=1.0,dtype=tf.float64)\n",
    "    rij = (tf.transpose(rij)+rij) / 2.0\n",
    "    rij = tf.multiply(rij,tf.constant([1.0],dtype=tf.float64)-tf.eye(N,dtype=tf.float64))\n",
    "\n",
    "    xij = tf.einsum('i,j->ij', xi, xi) # outer product\n",
    "    pij = xij / (1.0 + xij)\n",
    "    Amodel = 1.0 / (1.0 + tf.exp(50*rij-50*pij)) # this is like setting < to 0 and > to 1\n",
    "    Amodel = tf.multiply(Amodel,tf.constant([1.0],dtype=tf.float64)-tf.eye(shape[0],dtype=tf.float64)) # set diagonal to zero\n",
    "    # Compute the model density\n",
    "    model_density = tf.reduce_sum(Amodel)/(N * (N-1))\n",
    "    # Compute the model Laplacian    \n",
    "    Lmodel = tf.diag(tf.reduce_sum(Amodel,axis=0)) - Amodel\n",
    "\n",
    "    # Monitor the difference in number of links between model and data\n",
    "    deltaL = m - tf.reduce_sum(Amodel)\n",
    "\n",
    "    # Model energy\n",
    "    Em = tf.reduce_sum(tf.multiply(Lmodel,rho,name='Lmrho'),name='TrLmrho')\n",
    "    # Observation energy\n",
    "    Eo = tf.reduce_sum(tf.multiply(Lobs,rho,name='Lobsrho'),name='TrL_orho')\n",
    "    # Model laplacian eigenvalues\n",
    "    lm = tf.linalg.eigvalsh(Lmodel,name='lambda_model')\n",
    "    # Observation laplacian eigenvalues\n",
    "    lo = tf.linalg.eigvalsh(Lobs,name='lambda_obs')\n",
    "    # Model free energy\n",
    "    Fm = -tf.reduce_logsumexp(-beta*lm,name='Fm') / beta\n",
    "    # Observation free energy\n",
    "    Fo = -tf.reduce_logsumexp(-beta*lo,name='Fo') / beta\n",
    "\n",
    "    loglike = beta*(-Fm + Em)\n",
    "    entropy = beta*(-Fo + Eo)\n",
    "    rel_entropy = tf.abs(loglike - entropy)\n",
    "    grad = tf.gradients(rel_entropy,xi)\n",
    "\n",
    "    # Define the optimizer\n",
    "    learning_rate = tf.placeholder(dtype=tf.float64)\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(rel_entropy)\n",
    "    # Initialize the global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    all_loss = []\n",
    "    all_deltaM = []\n",
    "    beta_range = np.logspace(0,-2,10)\n",
    "    nepochs = 50\n",
    "    all_steps = 0\n",
    "    for b in beta_range:\n",
    "        for epoch in range(0,nepochs):\n",
    "            feed_dict = {Lobs:L, beta:b , learning_rate: 1}\n",
    "            sess.run(train, feed_dict = feed_dict)\n",
    "            res = sess.run([beta,rel_entropy,deltaL,model_density,learning_rate,xi],feed_dict = feed_dict)\n",
    "            sol = sess.run(xi,feed_dict = feed_dict)\n",
    "            all_loss.append(res[1])\n",
    "            all_deltaM.append(res[2])\n",
    "            all_steps += 1\n",
    "            # Logging\n",
    "            print(sol)\n",
    "            #print('\\rDone:%.1f%%\\tEpoch:%d\\tbeta: %.2g\\tLoss: %.2g\\tDeltaL: %.2f\\tDensity: %.2f\\tlearning_rate: %g' % (100.0*all_steps/(len(beta_range)*nepochs),epoch,res[0],res[1],res[2],res[3],res[4]), end='')\n",
    "    # Plotting part\n",
    "    fig,ax = plt.subplots(ncols=3,nrows=1,figsize=(24,8))\n",
    "    for i,b in enumerate(beta_range):\n",
    "        ax[0].plot(np.linspace(i*nepochs,(i+1)*nepochs,nepochs),all_loss[i*nepochs:(i+1)*nepochs])\n",
    "    ax[0].set_title('Relative entropy')\n",
    "    ax[0].set_xlabel('Iteration')\n",
    "    for i,b in enumerate(beta_range):\n",
    "        ax[1].plot(np.linspace(i*nepochs,(i+1)*nepochs,nepochs),all_deltaM[i*nepochs:(i+1)*nepochs])\n",
    "    ax[1].set_title('$\\\\Delta m$')\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    ax[2].bar(range(0,N),A.sum(axis=0) - np.diagonal(sess.run(Lmodel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,nrows=1,figsize=(24,8))\n",
    "for i,b in enumerate(beta_range):\n",
    "    ax[0].plot(all_loss)\n",
    "ax[0].set_title('Relative entropy')\n",
    "ax[0].set_xlabel('Iteration')\n",
    "for i,b in enumerate(beta_range):\n",
    "    ax[1].plot(np.linspace(i*nepochs,(i+1)*nepochs,nepochs),all_deltaM[i*nepochs:(i+1)*nepochs])\n",
    "ax[1].set_title('$\\\\Delta m$')\n",
    "ax[1].set_xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#pd.rolling_mean(pd.Series(all_deltaM),1).plot()\n",
    "pd.rolling_std(pd.Series(all_deltaM),5).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
