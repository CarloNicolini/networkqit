{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import networkqit as nq\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import numdifftools as nd\n",
    "from numpy.random import random as rand\n",
    "from networkqit import graph_laplacian as GL\n",
    "from scipy.linalg import eigvalsh\n",
    "from scipy.special import logsumexp\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=1.75)\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [ [[1., 2.,3.], [4.,5.,6.]], [[7.,8.,9.], [11.,12.,13.]] ]\n",
    "d = torch.Tensor(d) # array from python list\n",
    "print( \"shape of the tensor:\", d.size())\n",
    "\n",
    "# the first index is the depth\n",
    "z = d[0] + d[1]\n",
    "print( \"adding up the two matrices of the 3d tensor:\",z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a heavily used operation is reshaping of tensors using .view()\n",
    "print( d.view(2,-1)) #-1 makes torch infer the second dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "linear_map = nn.Linear(5,3)\n",
    "print( \"using randomly initialized params:\", linear_map.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data has 2 examples with 5 features and 3 target\n",
    "data = torch.randn(2,5) # training\n",
    "y = autograd.Variable(torch.randn(2,3)) # target\n",
    "# make a node\n",
    "x = autograd.Variable(data, requires_grad=True)\n",
    "# apply transformation to a node creates a computational graph\n",
    "a = linear_map(x)\n",
    "z = F.relu(a)\n",
    "o = F.softmax(z)\n",
    "print( \"output of softmax as a probability distribution:\", o.data.view(1,-1))\n",
    "\n",
    "# loss function\n",
    "loss_func = nn.MSELoss() #instantiate loss function\n",
    "L=loss_func(z,y) # calculateMSE loss between output and target\n",
    "print( \"Loss:\", L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
